{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqygHotEaBUL"
   },
   "source": [
    "# Neighbouring Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrbwytmXaM7e",
    "outputId": "fffc00c5-d065-4d4f-d0c6-78dd7821c76f"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "path = 'mypath'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2InrhQnaM7k"
   },
   "outputs": [],
   "source": [
    "df_metadata_train = pd.read_csv(path + 'df_metadata_train.csv', index_col=0)\n",
    "df_metadata_train.index = df_metadata_train.index.astype(str)\n",
    "\n",
    "df_metadata_test = pd.read_csv(path + 'df_metadata_test.csv', index_col=0)\n",
    "df_metadata_test.index = df_metadata_test.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8K7ehkJ-aM7m",
    "outputId": "37e7d053-ca79-46e3-d2c8-ae10ff708d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXDe-_50KHPu"
   },
   "outputs": [],
   "source": [
    "xr_train=xr.open_dataset(path+'xr_train_timestamped.netcdf')\n",
    "xr_test=xr.open_dataset(path+'xr_test_timestamped.netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGqRRXWFwiip"
   },
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "import pandas as pd\n",
    "\n",
    "def get_distances(coords, df_metadata_train):\n",
    "\n",
    "  distances = np.zeros(df_metadata_train.shape[0])\n",
    "\n",
    "  for i, ss in enumerate(df_metadata_train.index.values):\n",
    "\n",
    "      lat_train = df_metadata_train.loc[ss]['latitude_rounded']\n",
    "      lon_train = df_metadata_train.loc[ss]['longitude_rounded']\n",
    "\n",
    "      coords_train = (lat_train, lon_train)\n",
    "\n",
    "      distances[i] = geopy.distance.distance(coords, coords_train).km\n",
    "\n",
    "  return distances\n",
    "\n",
    "\n",
    "def df_of_neighbours(df_metadata, df_metadata_ref):\n",
    "  print(\"Generating df of distances\")\n",
    "  print('Progress (out of %d): ' %(df_metadata.shape[0]//50), end='')\n",
    "\n",
    "  res = np.zeros((df_metadata_ref.shape[0], df_metadata.shape[0]))\n",
    "\n",
    "  for i, ss in enumerate(df_metadata.index.values):\n",
    "    if i % 50 == 49: print('#', end='')\n",
    "    lat = df_metadata.loc[ss]['latitude_rounded']\n",
    "    lon = df_metadata.loc[ss]['longitude_rounded']\n",
    "    coords = (lat, lon)\n",
    "\n",
    "    res[:, i] = get_distances(coords, df_metadata_ref)\n",
    "\n",
    "  print('  Done')\n",
    "\n",
    "  return pd.DataFrame(data=res, index=df_metadata_ref.index.values, columns=df_metadata.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wF-xRYFElkjP",
    "outputId": "6ca00bbc-46ba-4072-b6f9-f83780f625f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating df of distances\n",
      "Progress (out of 13): #############  Done\n"
     ]
    }
   ],
   "source": [
    "df_distances_train = df_of_neighbours(df_metadata_train, df_metadata_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ia32tl7zluSQ",
    "outputId": "d6fc6334-c948-421c-c66a-3d4c39a781a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating df of distances\n",
      "Progress (out of 5): #####  Done\n"
     ]
    }
   ],
   "source": [
    "df_distances_test = df_of_neighbours(df_metadata_test, df_metadata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaMo5uXvE0iD"
   },
   "outputs": [],
   "source": [
    "df_distances_test.to_csv(path+'df_distances_test.csv')\n",
    "df_distances_train.to_csv(path+'df_distances_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2PxRObpH0cE"
   },
   "outputs": [],
   "source": [
    "df_distances_test = pd.read_csv(path+'df_distances_test.csv', index_col=0)\n",
    "df_distances_train = pd.read_csv(path+'df_distances_train.csv', index_col=0)\n",
    "df_distances_test.index = df_distances_test.index.astype(str)\n",
    "df_distances_train.index = df_distances_train.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V07jXCx8O5O1"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def get_seq_length(past_dict = {'year': 0, 'month': 0, 'week': 0, 'day': 1, 'hour': 0},\n",
    "                   future_dict = {'year': 0, 'month': 0, 'week': 0, 'day': 0, 'hour': 1},\n",
    "                   max_len = 133992):\n",
    "  look_up = (past_dict['hour'] * 4 + past_dict['day'] * 24 * 4\n",
    "            + past_dict['month'] *30 * 24 * 4 + past_dict['year'] * 12 * 30 * 24 * 4)\n",
    "  look_ahead = (future_dict['hour'] * 4 + future_dict['day'] * 24 * 4\n",
    "                + future_dict['month'] *30 * 24 * 4 + future_dict['year'] * 12 * 30 * 24 * 4)\n",
    "\n",
    "  if look_up+look_ahead > max_len:\n",
    "    print('Not enough data to accomodate requested sequence lengths, returning 1 and 1')\n",
    "    look_up, look_ahead = 1, 1\n",
    "\n",
    "  return look_up, look_ahead\n",
    "\n",
    "\n",
    "class PVDataset(Dataset):\n",
    "  def __init__(self, X, df_distances, look_up=1, look_ahead=1,\n",
    "               cut_off_km=64, n_neighbours=8):\n",
    "\n",
    "    '''\n",
    "    X = xarray dataset\n",
    "    look_up = int, length of sequence of past observations to use for prediction\n",
    "    look_ahead = int, length of sequence to predict\n",
    "    '''\n",
    "\n",
    "    self.X = X\n",
    "    self.stations = list(X.keys())\n",
    "    self.look_up = look_up\n",
    "    self.look_ahead = look_ahead\n",
    "    self.shape_0 = X.dims['datetime']\n",
    "    self.shape_1 = len(self.stations)\n",
    "    self.data_len = self.shape_0 // (self.look_up + self.look_ahead)\n",
    "    self.df_distances = df_distances\n",
    "    self.cut_off_km = cut_off_km\n",
    "    self.n_neighbours = n_neighbours\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.data_len * self.shape_1\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "\n",
    "    def get_neighbours(ss_id):\n",
    "      distances = self.df_distances[ss_id].sort_values().copy()\n",
    "      distances = distances[distances > self.cut_off_km].copy()\n",
    "\n",
    "      if len(distances) < self.n_neighbours:\n",
    "        return np.random.choice(distances.index.values, self.n_neighbours, replace=True)\n",
    "\n",
    "      else:\n",
    "        return distances.index.values[list(range(0, len(distances), len(distances)//self.n_neighbours))[:self.n_neighbours]]\n",
    "\n",
    "\n",
    "    ss_num = i // self.data_len\n",
    "    ss_id = self.stations[ss_num]\n",
    "    i = (i % self.shape_1) * (self.look_up + self.look_ahead)\n",
    "\n",
    "    t = i + self.look_up\n",
    "\n",
    "    neighbours = get_neighbours(ss_id)\n",
    "\n",
    "    x_item = np.zeros((self.look_up, 5+self.n_neighbours))\n",
    "    y_item = np.zeros((self.look_ahead, 1))\n",
    "\n",
    "    x_item[:, 0], y_item = self.X[ss_id].data[t-self.look_up : t], self.X[ss_id].data[t : t+self.look_ahead]\n",
    "\n",
    "    for i, ss in enumerate(neighbours):\n",
    "      x_item[:, i+1] = self.X[ss].data[t-self.look_up : t]\n",
    "\n",
    "    x_item[:, -4] = self.X.date_sin.data[t-self.look_up : t]\n",
    "    x_item[:, -3] = self.X.date_cos.data[t-self.look_up : t]\n",
    "    x_item[:, -2] = self.X.time_sin.data[t-self.look_up : t]\n",
    "    x_item[:, -1] = self.X.time_sin.data[t-self.look_up : t]\n",
    "\n",
    "    return torch.Tensor(x_item).float(), torch.Tensor(y_item).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b7enRgqO5O2"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def loaders(xr_test, xr_train, batch_size, dict_look_up, dict_look_ahead,\n",
    "            df_metadata_train, df_metadata_test,\n",
    "            cut_off_km=64, n_neighbours=8,\n",
    "            df_distances_train=None, df_distances_test=None):\n",
    "\n",
    "  if df_distances_train is None:\n",
    "    df_distances_train = df_of_neighbours(df_metadata_train, df_metadata_train).copy()\n",
    "  if df_distances_test is None:\n",
    "    df_distances_test = df_of_neighbours(df_metadata_test, df_metadata_train).copy()\n",
    "\n",
    "  max_len = len(xr_train['datetime'].data)\n",
    "\n",
    "  look_up, look_ahead = get_seq_length(past_dict=dict_look_up, future_dict=dict_look_ahead, max_len=max_len)\n",
    "\n",
    "  train_dataset = PVDataset(X=xr_train, df_distances=df_distances_train, look_up=look_up, look_ahead=look_ahead,\n",
    "                            cut_off_km=cut_off_km, n_neighbours=n_neighbours)\n",
    "  test_dataset = PVDataset(X=xr_test, df_distances=df_distances_test, look_up=look_up, look_ahead=look_ahead,\n",
    "                            cut_off_km=cut_off_km, n_neighbours=n_neighbours)\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "  print(\"Expected number of steps is %d\" %((train_dataset.__len__()  + batch_size - 1) // batch_size))\n",
    "\n",
    "  return train_loader, test_loader, look_ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bX5lCUbNaM7p"
   },
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "  def __init__(self, hidden_size,\n",
    "               num_layers,\n",
    "               out_features,\n",
    "               input_size,\n",
    "               dropout=0):\n",
    "\n",
    "    super().__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.lstm = nn.LSTM(input_size=input_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        num_layers=num_layers,\n",
    "                        batch_first=True,\n",
    "                        dropout=dropout\n",
    "                        )\n",
    "\n",
    "    self.fc = nn.Linear(in_features=hidden_size, out_features=out_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    h_t = torch.zeros(self.num_layers,\n",
    "                      batch_size,\n",
    "                      self.hidden_size).to(device)\n",
    "    c_t = torch.zeros(self.num_layers,\n",
    "                      batch_size,\n",
    "                      self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "    _, (h_out, _) = self.lstm(x, (h_t, c_t))\n",
    "\n",
    "    h_out = h_out[0].view(-1, self.hidden_size)\n",
    "\n",
    "    out = self.fc(h_out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60CTcF8baM7q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_function, print_every=500):\n",
    "  loss_array = []\n",
    "  model.train(True)\n",
    "  running_loss = 0.\n",
    "\n",
    "  for batch_index, batch in enumerate(train_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "    output = model(x_batch)\n",
    "\n",
    "    loss = loss_function(output, y_batch.squeeze())\n",
    "    running_loss += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_index % print_every == 0:\n",
    "      loss_array.append(running_loss/print_every)\n",
    "      print(\"Batch %d, loss: %f\" %(batch_index+1, running_loss/print_every))\n",
    "      running_loss=0.\n",
    "\n",
    "  loss_array.append(running_loss/((batch_index+1)%1000))\n",
    "\n",
    "  print()\n",
    "  return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRLRSp6SaM7r"
   },
   "outputs": [],
   "source": [
    "def validate_epoch(model, test_loader, loss_function):\n",
    "  model.train(False)\n",
    "  running_loss = 0.\n",
    "\n",
    "  for batch_index, batch in enumerate(test_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      output = model(x_batch)\n",
    "      loss = loss_function(output, y_batch.squeeze())\n",
    "      running_loss += loss.item()\n",
    "\n",
    "  avg_loss = running_loss / len(test_loader)\n",
    "\n",
    "  print('Val loss: {0:.5f}'.format(avg_loss))\n",
    "  print('_______________________________')\n",
    "  print()\n",
    "\n",
    "  return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_df = pd.read_csv(path+'neighbours_best_params.csv')\n",
    "best_params = best_params_df.loc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hA3uu6AVXwt",
    "outputId": "d77e5ba2-757a-4447-c03c-74947d0283f0"
   },
   "outputs": [],
   "source": [
    "n_layers = int(best_params['num_layers'])\n",
    "hid_size = int(best_params['hidden_size'])\n",
    "look_up = int(best_params['look_up'])\n",
    "dropout = best_params['dropout']\n",
    "cut_off_km = int(best_params['cut_off_km'])\n",
    "n_neighbours = int(best_params['n_neighbours'])\n",
    "\n",
    "\n",
    "data_params = {\n",
    "'xr_test': xr_test,\n",
    "'xr_train': xr_train,\n",
    "'xr_test': xr_test,\n",
    "'xr_train': xr_train,\n",
    "'batch_size': 128,\n",
    "'dict_look_up': {'year': 0, 'month': 0, 'week': 0, 'day': 0, 'hour': look_up},\n",
    "'dict_look_ahead': {'year': 0, 'month': 0, 'week': 0, 'day': 0, 'hour': 6},\n",
    "'cut_off_km': cut_off_km,\n",
    "'n_neighbours': n_neighbours,\n",
    "'df_metadata_train': df_metadata_train,\n",
    "'df_metadata_test': df_metadata_test,\n",
    "'df_distances_train': df_distances_train,\n",
    "'df_distances_test': df_distances_test\n",
    "}\n",
    "\n",
    "train_loader, test_loader, look_ahead = loaders(**data_params)\n",
    "\n",
    "model_params = {\n",
    "    'hidden_size': hid_size,\n",
    "    'num_layers': n_layers,\n",
    "    'out_features': look_ahead,\n",
    "    'input_size': n_neighbours+1+4,\n",
    "    'dropout': dropout\n",
    "}\n",
    "\n",
    "model = LSTM_model(**model_params)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 6\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "best_loss = 1\n",
    "best_epoch = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch:', epoch+1)\n",
    "    train_loss.append(train_epoch(model=model, train_loader=train_loader,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_function=loss_function,\n",
    "                                print_every=300))\n",
    "    test_loss.append(validate_epoch(model=model, test_loader=test_loader, loss_function=loss_function))\n",
    "    if test_loss[-1] < best_loss:\n",
    "        best_loss = test_loss[-1]\n",
    "        best_epoch = epoch\n",
    "    if epoch - best_epoch >= 2:\n",
    "        break\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), path+'neighbours_model_6ep')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wtFyfTaZcVUB",
    "UfWq69jVcVUG",
    "wVbwTql1h0P1",
    "pdDXxxJocVUP",
    "CemS6sOJjHes",
    "p34I-x72cVUT",
    "73rUY1QEdA41",
    "0KEVzJzeZym1"
   ],
   "gpuType": "V100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
