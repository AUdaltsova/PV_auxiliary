{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-y9AAOEcgsq"
   },
   "source": [
    "# ERA 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0tERHnEcgss",
    "outputId": "7b194cce-fa12-42cc-a77b-a4d4a5c690b4"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "\n",
    "path = 'mypath'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jd48ri6ucgsu"
   },
   "outputs": [],
   "source": [
    "df_metadata_train = pd.read_csv(path + 'df_metadata_train.csv', index_col=0)\n",
    "df_metadata_train.index = df_metadata_train.index.astype(str)\n",
    "\n",
    "df_metadata_test = pd.read_csv(path + 'df_metadata_test.csv', index_col=0)\n",
    "df_metadata_test.index = df_metadata_test.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmIXn2sdcgsv",
    "outputId": "82817cd7-d3a1-46c0-fc2c-9fb048fbb260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y1_S_ZGucgs5"
   },
   "outputs": [],
   "source": [
    "xr_train=xr.open_dataset(path+'train_sin_cos.netcdf')\n",
    "xr_test=xr.open_dataset(path+'test_sin_cos.netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BFjtvYD1cgs9"
   },
   "outputs": [],
   "source": [
    "df_distances_test = pd.read_csv(path+'df_distances_test.csv', index_col=0)\n",
    "df_distances_train = pd.read_csv(path+'df_distances_train.csv', index_col=0)\n",
    "df_distances_test.index = df_distances_test.index.astype(str)\n",
    "df_distances_train.index = df_distances_train.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4Re4R2IRcgs-"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def get_seq_length(past_dict = {'year': 0, 'month': 0, 'week': 0, 'day': 1, 'hour': 0},\n",
    "                   future_dict = {'year': 0, 'month': 0, 'week': 0, 'day': 0, 'hour': 1},\n",
    "                   max_len = 133992):\n",
    "  look_up = (past_dict['hour'] * 4 + past_dict['day'] * 24 * 4\n",
    "            + past_dict['month'] *30 * 24 * 4 + past_dict['year'] * 12 * 30 * 24 * 4)\n",
    "  look_ahead = (future_dict['hour'] * 4 + future_dict['day'] * 24 * 4\n",
    "                + future_dict['month'] *30 * 24 * 4 + future_dict['year'] * 12 * 30 * 24 * 4)\n",
    "\n",
    "  if look_up+look_ahead > max_len:\n",
    "    print('Not enough data to accomodate requested sequence lengths, returning 1 and 1')\n",
    "    look_up, look_ahead = 1, 1\n",
    "\n",
    "  return look_up, look_ahead\n",
    "\n",
    "\n",
    "class PVDataset_ERA(Dataset):\n",
    "  def __init__(self, X,\n",
    "               era, df_metadata, df_distances=None, look_up=1, look_ahead=1, with_neighbours=False,\n",
    "               cut_off_km=64, n_neighbours=8):\n",
    "\n",
    "    '''\n",
    "    X = xarray dataset\n",
    "    look_up = int, length of sequence of past observations to use for prediction\n",
    "    look_ahead = int, length of sequence to predict\n",
    "    era = xarray dataset of era5 data\n",
    "    '''\n",
    "\n",
    "    self.X = X\n",
    "    self.stations = list(X.keys())\n",
    "    self.look_up = look_up\n",
    "    self.look_ahead = look_ahead\n",
    "    self.shape_0 = X.dims['datetime']\n",
    "    self.shape_1 = len(self.stations)\n",
    "    self.data_len = self.shape_0 // (self.look_up + self.look_ahead)\n",
    "    self.df_metadata = df_metadata\n",
    "    self.era_keys = list(era.keys())\n",
    "    self.era = era\n",
    "\n",
    "    self.with_neighbours = with_neighbours\n",
    "    if with_neighbours:\n",
    "      self.df_distances = df_distances\n",
    "      self.cut_off_km = cut_off_km\n",
    "      self.n_neighbours = n_neighbours\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.data_len * self.shape_1\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "\n",
    "    def get_neighbours(ss_id):\n",
    "      distances = self.df_distances[ss_id].sort_values().copy()\n",
    "      distances = distances[distances > self.cut_off_km].copy()\n",
    "\n",
    "      if len(distances) < self.n_neighbours:\n",
    "        return np.random.choice(distances.index.values, self.n_neighbours, replace=True)\n",
    "\n",
    "      else:\n",
    "        return distances.index.values[list(range(0, len(distances), len(distances)//self.n_neighbours))[:self.n_neighbours]]\n",
    "\n",
    "\n",
    "    ss_num = i // self.data_len\n",
    "    ss_id = self.stations[ss_num]\n",
    "    i = (i % self.shape_1) * (self.look_up + self.look_ahead)\n",
    "\n",
    "    t = i + self.look_up\n",
    "\n",
    "    while sum(self.X[ss_id].data[t-self.look_up : t]) < 0.5:\n",
    "      i = np.random.choice(range(self.__len__()), 1)[0]\n",
    "\n",
    "      ss_num = i // self.data_len\n",
    "      ss_id = self.stations[ss_num]\n",
    "      i = (i % self.shape_1) * (self.look_up + self.look_ahead)\n",
    "\n",
    "      t = i + self.look_up\n",
    "\n",
    "    x_item = np.zeros((self.look_up, 5+len(self.era_keys)))\n",
    "    y_item = np.zeros((self.look_ahead, 1))\n",
    "\n",
    "    if self.with_neighbours:\n",
    "      x_item = np.zeros((self.look_up, 5+len(self.era_keys)+self.n_neighbours))\n",
    "      neighbours = get_neighbours(ss_id)\n",
    "\n",
    "    x_item[:, 0], y_item = self.X[ss_id].data[t-self.look_up : t], self.X[ss_id].data[t : t+self.look_ahead]\n",
    "\n",
    "    era_slice = self.era.sel(datetime=self.X['datetime'].data[t-self.look_up : t], ss_id=ss_id).copy()\n",
    "\n",
    "    for i, key in enumerate(self.era_keys):\n",
    "      x_item[:, i+1] = era_slice[key].data\n",
    "\n",
    "    if self.with_neighbours:\n",
    "      for i, ss in enumerate(neighbours):\n",
    "        x_item[:, i+1+len(self.era_keys)] = self.X[ss].data[t-self.look_up : t]\n",
    "\n",
    "    x_item[:, -4] = self.X.date_sin.data[t-self.look_up : t]\n",
    "    x_item[:, -3] = self.X.date_cos.data[t-self.look_up : t]\n",
    "    x_item[:, -2] = self.X.time_sin.data[t-self.look_up : t]\n",
    "    x_item[:, -1] = self.X.time_sin.data[t-self.look_up : t]\n",
    "\n",
    "    return torch.Tensor(x_item).float(), torch.Tensor(y_item).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BWCRpeQIcgs_"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def loaders(xr_test, xr_train, batch_size, dict_look_up, dict_look_ahead,\n",
    "            df_metadata_train, df_metadata_test,\n",
    "            cut_off_km=64, n_neighbours=8,\n",
    "            df_distances_train=None, df_distances_test=None,\n",
    "            demo=False, with_neighbours=False, era=None):\n",
    "\n",
    "  if with_neighbours:\n",
    "    if df_distances_train is None:\n",
    "      df_distances_train = df_of_neighbours(df_metadata_train, df_metadata_train).copy()\n",
    "    if df_distances_test is None:\n",
    "      df_distances_test = df_of_neighbours(df_metadata_test, df_metadata_train).copy()\n",
    "\n",
    "  max_len = len(xr_train['datetime'].data)\n",
    "\n",
    "  look_up, look_ahead = get_seq_length(past_dict=dict_look_up, future_dict=dict_look_ahead, max_len=max_len)\n",
    "\n",
    "  train_dataset = PVDataset_ERA(X=xr_train, df_distances=df_distances_train, look_up=look_up, look_ahead=look_ahead,\n",
    "                            cut_off_km=cut_off_km, n_neighbours=n_neighbours,\n",
    "                            df_metadata=df_metadata_train, era=era, with_neighbours=with_neighbours)\n",
    "  test_dataset = PVDataset_ERA(X=xr_test, df_distances=df_distances_test, look_up=look_up, look_ahead=look_ahead,\n",
    "                            cut_off_km=cut_off_km, n_neighbours=n_neighbours,\n",
    "                            df_metadata=df_metadata_test, era=era, with_neighbours=with_neighbours)\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "  print(\"Expected number of steps is %d\" %((train_dataset.__len__()  + batch_size - 1) // batch_size))\n",
    "\n",
    "  return train_loader, test_loader, look_ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HRNbEIu1cgs_"
   },
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "  def __init__(self, hidden_size,\n",
    "               num_layers,\n",
    "               out_features,\n",
    "               input_size,\n",
    "               dropout=0):\n",
    "\n",
    "    super().__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.lstm = nn.LSTM(input_size=input_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        num_layers=num_layers,\n",
    "                        batch_first=True,\n",
    "                        dropout=dropout\n",
    "                        )\n",
    "\n",
    "    self.fc = nn.Linear(in_features=hidden_size, out_features=out_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    h_t = torch.zeros(self.num_layers,\n",
    "                      batch_size,\n",
    "                      self.hidden_size).to(device)\n",
    "    c_t = torch.zeros(self.num_layers,\n",
    "                      batch_size,\n",
    "                      self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "    _, (h_out, _) = self.lstm(x, (h_t, c_t))\n",
    "\n",
    "    h_out = h_out[0].view(-1, self.hidden_size)\n",
    "\n",
    "    out = self.fc(h_out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dXmhWm9scgtA"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_function, print_every=500):\n",
    "  loss_array = []\n",
    "  model.train(True)\n",
    "  running_loss = 0.\n",
    "\n",
    "  for batch_index, batch in enumerate(train_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "    output = model(x_batch)\n",
    "\n",
    "    loss = loss_function(output, y_batch.squeeze())\n",
    "    running_loss += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_index % print_every == 0:\n",
    "      loss_array.append(running_loss/print_every)\n",
    "      print(\"Batch %d, loss: %f\" %(batch_index+1, running_loss/print_every))\n",
    "      running_loss=0.\n",
    "\n",
    "  loss_array.append(running_loss/((batch_index+1)%1000))\n",
    "\n",
    "  print()\n",
    "  return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oyKg5_JFcgtA"
   },
   "outputs": [],
   "source": [
    "def validate_epoch(model, test_loader, loss_function):\n",
    "  model.train(False)\n",
    "  running_loss = 0.\n",
    "\n",
    "  for batch_index, batch in enumerate(test_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      output = model(x_batch)\n",
    "      loss = loss_function(output, y_batch.squeeze())\n",
    "      running_loss += loss.item()\n",
    "\n",
    "  avg_loss = running_loss / len(test_loader)\n",
    "\n",
    "  print('Val loss: {0:.5f}'.format(avg_loss))\n",
    "  print('_______________________________')\n",
    "  print()\n",
    "\n",
    "  return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hyF_7rECGFir"
   },
   "outputs": [],
   "source": [
    "best_params_df = pd.read_csv(path+'era_neighbours_best_params.csv')\n",
    "best_params = best_params_df.loc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GLJ2wwyGNk5",
    "outputId": "21b57552-0809-4601-dbd9-9be69722554c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 2.0,\n",
       " 'hidden_size': 256.0,\n",
       " 'look_up': 18.0,\n",
       " 'dropout': 0.9905943443390038,\n",
       " 'cut_off_km': 128.0,\n",
       " 'n_neighbours': 7.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvsaH1HycgtD",
    "outputId": "f899cc96-6b71-46df-b91d-8d1689156a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of steps is 2289\n"
     ]
    }
   ],
   "source": [
    "n_neighbours = int(best_params['n_neighbours'])\n",
    "\n",
    "data_params = {\n",
    "    'xr_test': xr_test_sample,\n",
    "    'xr_train': xr_train_sample,\n",
    "    'batch_size': 128,\n",
    "    'dict_look_up': {'year': 0, 'month': 0, 'week': 0, 'day': 0, 'hour': int(best_params['look_up'])},\n",
    "    'dict_look_ahead': {'year': 0, 'month': 0, 'week': 0, 'day': 0, 'hour': 6},\n",
    "    'cut_off_km': int(best_params['cut_off_km']),\n",
    "    'n_neighbours': n_neighbours,\n",
    "    'df_metadata_train': df_metadata_train,\n",
    "    'df_metadata_test': df_metadata_test,\n",
    "    'df_distances_train': df_distances_train_sample,\n",
    "    'df_distances_test':df_distances_test_sample,\n",
    "    'with_neighbours': True,\n",
    "    'era': era\n",
    "    }\n",
    "\n",
    "train_loader, test_loader, look_ahead = loaders(**data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-92ntWM5cgtF",
    "outputId": "765ec6e0-840d-4af4-b649-c3b5398b96cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_model(\n",
       "  (lstm): LSTM(15, 256, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = {\n",
    "    'hidden_size': int(best_params['hidden_size']),\n",
    "    'num_layers': int(best_params['num_layers']),\n",
    "    'out_features': look_ahead,\n",
    "    'input_size': n_neighbours+1+4+3,\n",
    "    'dropout': int(best_params['dropout'])\n",
    "}\n",
    "\n",
    "model = LSTM_model(**model_params)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3hqaVd4ApA1",
    "outputId": "5a1014a8-136e-4912-f561-41a545a56273"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 10\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(\"Epoch %d:\" %(epoch+1))\n",
    "  train_loss.append(train_epoch(model, train_loader, optimizer, loss_function, print_every=100))\n",
    "  test_loss.append(validate_epoch(model, test_loader, loss_function))\n",
    "\n",
    "torch.save(model.state_dict(), path+'ERA_neighbours_model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wtFyfTaZcVUB",
    "UfWq69jVcVUG",
    "wVbwTql1h0P1",
    "pdDXxxJocVUP",
    "CemS6sOJjHes",
    "p34I-x72cVUT",
    "73rUY1QEdA41",
    "0KEVzJzeZym1"
   ],
   "gpuType": "V100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
